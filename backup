import cv2
import torch
from ibm_watson import NaturalLanguageUnderstandingV1
from ibm_cloud_sdk_core.authenticators import IAMAuthenticator
from ultralytics import YOLO

# Configurar o IBM Watson NLU para análise de emoções
def configurar_watson():
    authenticator = IAMAuthenticator('UKd159aKfpf-7028jIvsxFL0G8jxLd_M7-IyX0A41m9V')  # Substitua pela sua chave de API
    nlu = NaturalLanguageUnderstandingV1(
        version='2022-03-10',
        authenticator=authenticator
    )
    nlu.set_service_url('https://api.au-syd.natural-language-understanding.watson.cloud.ibm.com/instances/b050e8dd-7433-4d69-a657-8b333ed2749a')  # Substitua pela URL do seu serviço Watson
    return nlu

# Função para obter a descrição emocional da pessoa via Watson
def obter_descricao_emocao_ibm(texto):
    try:
        response = nlu.analyze(
            text=texto,
            features={'emotion': {}}
        ).get_result()
        emocao = response.get('emotion', {}).get('document', {}).get('emotion', {})
        return emocao
    except Exception as e:
        return {"erro": "Não foi possível analisar a emoção"}

# Função de reconhecimento de objetos (sem Watson)
def reconhecer_objetos(frame, model, device):
    # Processar a imagem para detectar objetos
    frame_resized = cv2.resize(frame, (640, 640))  # Ajustar a resolução para o modelo
    frame_resized = frame_resized / 255.0  # Normalizar a imagem
    frame_tensor = torch.from_numpy(frame_resized).float().to(device)
    frame_tensor = frame_tensor.permute(2, 0, 1).unsqueeze(0)  # Transformar em BCHW

    # Enviar para o modelo YOLO
    results = model(frame_tensor)  # Obtenha os resultados diretamente
    detections = results[0].boxes  # Obter as caixas de detecção

    # Filtro para detecções
    objetos_detectados = []
    for det in detections:
        x1, y1, x2, y2 = det.xyxy[0].tolist()  # Coordenadas da caixa
        score = det.conf[0].item()  # Pontuação de confiança
        class_id = int(det.cls[0].item())  # ID da classe

        if score > 0.6:  # Apenas objetos com confiança acima de 0.6
            nome_objeto = model.names[class_id]  # Nome do objeto
            objetos_detectados.append((nome_objeto, (x1, y1, x2, y2), score))
    
    return objetos_detectados

# Inicializar o modelo YOLO
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = YOLO('yolov5s.pt').to(device)

# Iniciar captura de vídeo
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FPS, 30)  # Definir FPS
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  # Ajustar resolução
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

# Configurar Watson para analisar emoções
nlu = configurar_watson()

# Loop de captura de vídeo
while True:
    ret, frame = cap.read()
    if not ret:
        break  # Se não conseguir capturar, sair do loop

    # Reconhecer objetos
    objetos = reconhecer_objetos(frame, model, device)

    # Processar emoções (apenas quando a pessoa for identificada)
    texto_emocional = "I am very happy!"  # Exemplo de texto para analisar a emoção
    emocao = obter_descricao_emocao_ibm(texto_emocional)

    # Exibir resultados
    for objeto_nome, (x1, y1, x2, y2), score in objetos:
        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
        texto = f"{objeto_nome} ({score:.2f})"
        cv2.putText(frame, texto, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Exibir a emoção detectada pela Watson
    if "erro" not in emocao:
        # Adicionar um controle de exibição da emoção
        emocao_texto = ", ".join([f"{k}: {v:.2f}" for k, v in emocao.items()])
        cv2.putText(frame, f"Emotion: {emocao_texto}", (20, 460), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
    else:
        cv2.putText(frame, "Error analyzing emotion", (20, 460), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)

    # Mostrar a imagem com as detecções e emoções
    cv2.imshow('Object Recognition - YOLOv5', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break  # Pressione 'q' para sair

cap.release()
cv2.destroyAllWindows()
